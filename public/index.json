[
{
	"uri": "http://localhost:1313/nhatnm.github.io/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Accelerating the Quantum Toolbox for Python (QuTiP) with cuQuantum on AWS by Boris Varbanov, Benchen Huang, Éric Giguère, Jin-Sung Kim, Khaldoon Ghanem, Tyler Takeshita, and Timothy Brown | AUGUST 9, 2025 | in Amazon Braket, Quantum Technologies\nSimulating quantum systems on classical computers remains a computational challenge. In fact, the resources required for such simulations grow exponentially with the size of the system. This reality is central to the motivation for building quantum computers. Quantum computers have the potential to outperform even the most powerful supercomputers when simulating quantum systems found in nature. They are also expected to speed up the solution of certain complex problems, with applications ranging from cryptography to large-scale optimization. However, solving real-world problems of interest will require algorithms that perform billions of operations on hundreds of thousands of qubits. Current quantum hardware is far from meeting these demands: the number of qubits is still limited, and gate error rates remain high. Scientists and engineers worldwide are working to improve device design and operation to enable large-scale quantum computing.\nDesigning high-performance classical simulations for quantum devices is an active research field at the intersection of experimental science and computational science. Researchers often simulate a small portion of the system to obtain results within a reasonable time—for example, focusing on low-lying excited states and a few qubits or couplers. However, scientists increasingly recognize that a more comprehensive understanding of underlying physical systems is needed to improve hardware performance and reduce error rates. This requires incorporating more complex physical effects, such as interactions with other qubits on the device or the impact of higher energy states. Yet these more realistic simulations are computationally expensive.\nIn this post, researchers from the Superconducting Quantum Device Theory group at the Institut quantique, Université de Sherbrooke, NVIDIA, and Amazon Web Services (AWS) collaborated to accelerate open-quantum-system simulations using classical compute resources. The team achieved this by integrating NVIDIA cuQuantum with the Quantum Toolbox in Python (QuTiP), enabling GPU-accelerated simulation of quantum device dynamics. Leveraging Amazon Elastic Compute Cloud (Amazon EC2) instances powered by NVIDIA GPUs, we demonstrate simulation speedups of up to 4000×, enabling large-scale multi-qubit system modeling with improved performance.\nThe qutip-cuquantum Plugin QuTiP is an open-source software toolbox for simulating the dynamics of open quantum systems. It aims to provide efficient and user-friendly numerical simulations for a variety of Hamiltonians, including arbitrarily time-dependent ones commonly found in quantum optics, trapped ions, superconducting circuits, and quantum nanomechanical resonators. NVIDIA cuQuantum is a software development kit (SDK) containing optimized libraries and tools designed to accelerate quantum computing simulations at both the circuit and device levels.\nTo expand the range of computational experiments, our team integrated cuQuantum with QuTiP through the qutip-cuquantum plugin, available on PyPI. This plugin leverages NVIDIA’s new cuQuantum library, cuDensityMat, designed to accelerate analog quantum dynamics solvers and provide GPU-accelerated time evolution for the Schrödinger equation and the Lindblad master equation. cuDensityMat provides primitives that accelerate existing dynamics frameworks such as NVIDIA CUDA-Q Dynamics and now QuTiP, as well as accelerating custom-built solvers. It offers low-level functionality to define arbitrary pure or mixed quantum states, specify multi-body operators and superoperators, and compute their action on states. It also supports gradients, multi-GPU, and multi-node simulations for easy scaling.\nResults To evaluate the performance of the qutip-cuquantum plugin, we simulated a superconducting transmon qubit capacitively coupled to a resonator and driven by a microwave pulse. The system operates in the dispersive regime, where the qubit-resonator coupling strength is much smaller than their detuning. This configuration is a standard approach for qubit readout in superconducting processors. In principle, increasing the microwave drive amplitude can shorten measurement time. In practice, however, it often induces unwanted qubit state transitions, pushing the system beyond the computational subspace. Accurately capturing these effects requires modeling many qubit and resonator states, dramatically increasing the size of the Hilbert space and making the dynamics simulation computationally demanding. Here, we report results for systems with 512 resonator states and 32 or 64 qubit states, with the full Hamiltonian constructed in QuTiP.\nFigure 1 – Circuit diagram of the simulated system, where the transmon qubit (green) is capacitively coupled to the resonator (blue) with an external drive. The team ran simulations on AWS using P4de, P5, and P5en EC2 instances powered by NVIDIA A100, NVIDIA H100, and NVIDIA H200 GPUs, respectively. Benchmarks show reduced runtime for the 32-qubit-state system, with a speedup of 725× on a single H200 GPU compared to a CPU-only simulation on an Hpc7a instance. Scaling to multiple H200 GPUs yields additional speedups of 1.2×, 2.2×, and 3.7× for 2, 4, and 8 GPUs, respectively. The 64-qubit-state simulation requires 8 H200 GPUs due to memory needs and achieved a 4000× speedup on the P5en instance with 8 GPUs. Finally, we observed GPU-generation improvements, with speedups of 1.5× and 1.9× when moving from P4de to P5 and P5en.\nThese advancements open new opportunities for studying multi-qubit dynamics and interactions involving highly excited states, which are crucial for optimizing operations such as readout and two-qubit gates. With these new tools, the research team can now explore transmon physics at scales previously out of reach—marking an important step toward next-generation quantum hardware. Integrating GPU acceleration into QuTiP also ensures these capabilities are broadly accessible to the research community.\nTAGS: Amazon Braket, NVIDIA, quantum computing, quantum research, quantum technologies\nBoris Varbanov Boris Varbanov is a postdoctoral researcher at the Institut quantique, Université de Sherbrooke, where he studies how superconducting qubits can be excited outside their computational subspace. His work includes simulating the impact of such errors on quantum error-correction codes and developing methods or hardware design improvements to mitigate these errors. He received his Ph.D. in Physics from Delft University of Technology. His research focuses on enabling quantum error correction in superconducting processors, and he often collaborates closely with experimental teams working toward this goal. Benchen Huang Benchen Huang is a Specialist Solutions Architect for Quantum Computing at AWS, working with customers to develop hybrid quantum-classical solutions on the cloud. He holds a Ph.D. in Chemistry from the University of Chicago. His research focuses on applying quantum computing to chemistry problems. Éric Giguère Éric Giguère is a Research Specialist at the Université de Sherbrooke. He is a maintainer of the open-source QuTiP project. He holds a Ph.D. from Hokkaido University, where he studied theoretical particle physics. Jin-Sung Kim Jin-Sung Kim is a Quantum Developer Relations Manager at NVIDIA, overseeing strategic partnerships and alliances. Before joining NVIDIA, he was a Research Staff Member at IBM Quantum. Jin-Sung holds a Ph.D. in Electrical Engineering and Materials Science from Princeton University and is currently based in San Francisco. Khaldoon Ghanem Khaldoon Ghanem is a Senior Development Technology Engineer at NVIDIA, specializing in quantum computing. He holds a Ph.D. in computational quantum physics from RWTH Aachen University, in collaboration with the Jülich Supercomputing Centre. Prior to NVIDIA, he was a researcher at Quantinuum, developing quantum algorithms for condensed-matter physics applications. He also spent several years at the Max Planck Institute in Stuttgart, working on large-scale parallel quantum Monte Carlo simulations. Tyler Takeshita Tyler Takeshita is a Senior Applied Scientist for Quantum Computing at AWS. Before joining AWS, he was Head of Quantum Technology at the Mercedes-Benz Research and Development North America, a subsidiary of Daimler. Tyler received his Ph.D. in Chemistry in 2015 from the University of Illinois at Urbana-Champaign, specializing in theoretical quantum chemistry. He later conducted postdoctoral research in the Department of Chemistry at the University of California, Berkeley. Timothy Brown Timothy Brown is a Principal Solutions Architect for Computing \u0026 HPC at AWS. He has worked in High-Performance Computing for the past 15 years, serving in various roles involving simulation and optimization, particularly in numerical weather prediction. Timothy holds a Master’s and a Bachelor’s (Honors) degree from the University of Western Australia (UWA). "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "New General-Purpose Amazon EC2 M8i and M8i-flex Instances Are Now Available by Channy Yun (윤석찬) | AUGUST 28, 2025 | in Amazon EC2, Compute, Launch, News\nda4b9237bacccdf19c0760cab7aec4a8359010b0amazon_polly_98931.mp3\nToday, we are excited to announce the general availability of the new general-purpose Amazon Elastic Compute Cloud (Amazon EC2) instances, M8i and M8i-flex, powered by custom-built Intel Xeon 6 processors available exclusively on AWS—with a consistent all-core turbo frequency of 3.9 GHz. These instances provide the highest performance and fastest memory bandwidth among comparable Intel processors on any cloud platform. They also deliver up to 15% better price performance, up to 20% higher compute performance, and 2.5× greater memory bandwidth than the previous-generation M7i and M7i-flex instances.\nM8i and M8i-flex instances are ideal for general-purpose workloads such as web application servers, virtual desktops, batch processing, microservices, databases, and enterprise applications. In terms of performance, they are up to 60% faster for NGINX web applications, up to 30% faster for PostgreSQL database workloads, and up to 40% faster for deep learning recommendation models compared to M7i and M7i-flex.\nLike the R8i and R8i-flex instances, these new instances use the sixth-generation AWS Nitro Cards, offering twice the network and Amazon Elastic Block Store (Amazon EBS) bandwidth of the previous generation. This significantly improves network throughput for workloads that process small packets such as web servers, applications, and gaming servers. They also support configurable bandwidth with a 25% reallocation capability between network and EBS bandwidth, enabling better database performance, improved query processing, and faster log write speeds.\nM8i Instances These instances offer up to 384 vCPUs and 1.5 TB of memory, including bare-metal options providing direct access to underlying physical hardware. SAP-certified, these instances allow you to run large application servers, databases, game servers, CPU-based inference, and video streaming workloads that require the largest instance sizes or consistently high CPU performance.\nBelow are the specifications for M8i instances:\nInstance Size vCPU Memory (GiB) Network Bandwidth (Gbps) EBS Bandwidth (Gbps) m8i.large 2 8 Up to 12.5 Up to 10 m8i.xlarge 4 16 Up to 12.5 Up to 10 m8i.2xlarge 8 32 Up to 15 Up to 10 m8i.4xlarge 16 64 Up to 15 Up to 10 m8i.8xlarge 32 128 15 10 m8i.12xlarge 48 192 22.5 15 m8i.16xlarge 64 256 30 20 m8i.24xlarge 96 384 40 30 m8i.32xlarge 128 512 50 40 m8i.48xlarge 192 768 75 60 m8i.96xlarge 384 1536 100 80 m8i.metal-48xl 192 768 75 60 m8i.metal-96xl 384 1536 100 80 M8i-flex Instances These are lower-cost variants of the M8i instances, offering 5% higher performance and 5% lower prices. They are designed for workloads that benefit from next-generation performance but do not fully utilize the compute resources. These instances can achieve maximum CPU performance 95% of the time.\nBelow are the specifications for M8i-flex instances:\nInstance Size vCPU Memory (GiB) Network Bandwidth (Gbps) EBS Bandwidth (Gbps) m8i-flex.large 2 8 Up to 12.5 Up to 10 m8i-flex.xlarge 4 16 Up to 12.5 Up to 10 m8i-flex.2xlarge 8 32 Up to 15 Up to 10 m8i-flex.4xlarge 16 64 Up to 15 Up to 10 m8i-flex.8xlarge 32 128 Up to 15 Up to 10 m8i-flex.12xlarge 48 192 Up to 22.5 Up to 15 m8i-flex.16xlarge 64 256 Up to 30 Up to 20 If you currently use earlier generations of general-purpose instances, you can adopt M8i-flex without modifying your applications or workloads.\nAmazon EC2 M8i and M8i-flex instances are now available in the AWS Regions US East (N. Virginia), US East (Ohio), US West (Oregon), and Europe (Spain). You can purchase M8i and M8i-flex as On-Demand Instances, Savings Plans, or Spot Instances. M8i is also available as a Dedicated Instance and on Dedicated Hosts. For more information, visit the Amazon EC2 Pricing page.\nTry the M8i and M8i-flex instances in the Amazon EC2 Console. To learn more, visit the Amazon EC2 M8i Instance page and send feedback through AWS re:Post for EC2 or your usual AWS Support contacts.\n— Channy\nAuthor Channy Yun (윤석찬) Channy is the Lead Blogger for the AWS News Blog and Principal Developer Advocate for AWS Cloud. A passionate open-web enthusiast and long-time blogger, he loves learning and sharing community-driven technologies. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "New courses and certifications updated by AWS Training and Certification in August 2025 by Training and Certification Blog Editors | AUGUST 28, 2025 | in Announcements, AWS Training and Certification\nWelcome to the August post announcing newly launched training and certification offerings—helping equip you and your team with the skills to work with AWS services and solutions. Did you miss our July course update? View it here.\nThis month, we launched nine new digital training products on AWS Skill Builder, including localized versions of the Exam Prep materials for AWS Certified AI Practitioner and AWS Certified Cloud Practitioner, as well as a major update to AWS Jam that allows Skill Builder Team subscription users (not just Administrators) to create Jam events!\nNew AWS Skill Builder subscription features The AWS Skill Builder subscription is available globally and unlocks the advanced AWS Certification Exam Prep program along with hands-on AWS Cloud training, including interactive and practical learning experiences such as AWS Cloud Quest, AWS Industry Quest, AWS Builder Labs, and AWS Jam challenges. AWS Digital Classroom courses offer deeper learning with expert-led instruction and are available with the Annual Individual subscription or the Team subscription.\nSee the latest additions and updates to the AWS Skill Builder subscription below:\nAWS Jam\nWe are excited to announce that Jam event creation is no longer reserved only for administrators but is now available to all Skill Builder Team users! Spark innovation, encourage healthy competition, and watch your team’s cloud expertise reach new heights—all completely free. All current Skill Builder Team Subscription users can now easily schedule custom Jam events with up to 20 participants and 6 challenges at jam.aws.com.\nPreparation and updates for AWS Certification exams Certification Additional languages for foundational-level certification exams are now available.\nCandidates can now take the AWS Certified AI Practitioner exam in the following additional languages: Arabic, French, German, Italian, Spanish (Latin America), Spanish (Spain), and Traditional Chinese. In addition, the AWS Certified Cloud Practitioner exam is now available in Arabic.\nGet exam-ready with AWS Skill Builder:\nThe Exam Prep Plan: AWS Certified AI Practitioner and the Exam Prep Plan: AWS Certified Cloud Practitioner are now available in the additional languages listed above.\nThe exam prep plans include practice assessments with exam-style questions, hands-on practice via AWS SimuLearn or AWS Builder Labs, and review lessons for each exam domain and task statement.\nAWS Classroom Training AWS Classroom Training provides live classes with instructors who teach you in-demand cloud skills and best practices through a mix of presentations, discussions, and hands-on labs. Ask questions, explore solutions in real time, and receive feedback from AWS-authorized instructors with deep subject-matter expertise.\nThis month, we launched the Agentic AI Foundations course. This 1-day foundational course explores the core principles and strategies for designing Agentic AI systems using AWS services. You will learn the differences between Agentic AI and traditional conversational systems, and how to use tools such as Amazon Q, Kiro, Amazon Bedrock Agents, and Amazon Bedrock AgentCore to build autonomous, goal-oriented solutions that solve real-world problems.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Event Information Date \u0026amp; Time: Thursday, September 18, 2025, 9:00 AM – 5:00 PM Location: Ho Chi Minh City Role: Attendee Event Objectives Opening keynote: AWS presented its strategic vision for Vietnam and opportunities for young builders. Introduced the rise of Agentic AI and AWS’s strategy for the next phase. Explained how to build a unified data foundation that supports analytics and AI workloads. Detailed the GenAI roadmap, AI Agent architectures, and challenges when moving to production. Provided insights into the AI-Driven Development Lifecycle (AI-DLC). Highlighted security principles, risk governance, and Responsible AI practices. Introduced new AWS services for AI Agents and enterprise productivity. Speaker List Hon. – Government Speaker Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Jens Lottner – CEO, Techcombank Trang Phung – CEO \u0026amp; Co-Founder, U2U Network Jaime Valles – Vice President, General Manager Asia Pacific and Japan, AWS Jeff Johnson – Managing Director, ASEAN, AWS Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, TymeX Jun Kai Loke – AI/ML Specialist Solutions Architect, AWS Kien Nguyen – Solutions Architect, AWS Tamelly Lim – Storage Specialist Solutions Architect, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Christal Poon – Specialist Solutions Architect, AWS Key Highlights Opening – Agentic AI Overview – Jun Kai Loke Agentic AI Trends Agentic AI represents the next evolution of AI—focusing on autonomous systems that reduce human supervision. Real-world examples: Katalon, Apero, Techcom Securities. Amazon Bedrock – Enterprise AI Platform Enterprise-grade security Tool + memory integration End-to-end observability and governance Building a Unified Data Foundation on AWS – Kien Nguyen Key Challenges 89% of CDOs have initiated GenAI adoption, but only 52% believe their data foundation is ready. Data, people, and operational silos are the primary barriers. Three layers of an End-to-End Data Architecture Producers Foundations Consumers Critical AWS Data Components Amazon Bedrock Databases – RDS and vector-capable databases Analytics \u0026amp; ML – SageMaker, Unified Studio Data Governance Lake House – S3, Redshift Managed Storage, Iceberg Amazon DataZone GenAI Roadmap \u0026amp; AI Agents Architecture – Jun Kai Loke \u0026amp; Tamelly Lim The Agent framework includes model capabilities, application abilities, and tool integration. Key components introduced: Amazon Bedrock Amazon Nova Strands Agents Bedrock AgentCore AgentCore includes: Agent Core Runtime Agent Core Gateway Memory Engine Agent Browser Code Interpreter → Enhances security, scalability, and operational reliability.\nAI-Driven Development Lifecycle (AI-DLC) – Binh Tran Two Development Patterns Today AI Managed Pattern – Minimal supervision; reliability depends heavily on the model AI Assisted Pattern – AI supports individual tasks rather than whole workflows Three AI-DLC Stages Inception – Context building, user stories, planning work units Construction – Coding, testing, architectural refinement, IaC + validation Operation – Production deployment via IaC and incident handling Securing Generative AI Applications – Taiki Dang Core Security Considerations Compliance \u0026amp; Governance Legal \u0026amp; Privacy Controls Risk Management Resilience Scoping Matrix Consumer Apps Enterprise Apps Pre-trained Models Fine-tuned Models Custom-trained Models Frameworks \u0026amp; Standards AWS Well-Architected MITRE ATLAS OWASP Top 10 for LLM Apps NIST AI 600-1 ISO 42001 EU AI Act Major Risk Layers Consumer: IP, legal, hallucination risks Tuner: Data retention, managed infrastructure concerns Provider: Training datasets, model construction Risk Mitigation Techniques Prompt engineering Fine-tuning RAG Parameter tuning Bedrock Guardrails Prompt security Beyond Automation: AI Agents as Productivity Multipliers – Christal Poon Types of AI Agents Specialized Agents Fully-managed Agents DIY Agents Enterprise Productivity Tools Amazon QuickSight Amazon Q – Dashboards, reports, summaries, task automation Coming Soon to Vietnam – QuickSuite Quick Researcher Quick Automate Humans in the Loop Key Takeaways Clear understanding of the Agentic AI ecosystem and AWS roadmap. Recognizing that data foundations (S3, Iceberg, Redshift, Bedrock, SageMaker) are the backbone of GenAI. AI-DLC introduces a modern, high-automation SDLC approach. Security must be embedded end-to-end across the AI stack. AWS is rapidly expanding the enterprise AI Agent ecosystem. Applications to Work Integrating AI Agents into operational workflows. Using Bedrock, Amazon Q, and Guardrails to ensure safety and quality. Building a proper data foundation before deploying GenAI workloads. Applying AI-DLC to internal development pipelines. Creating dashboards and insights using QuickSight \u0026amp; Amazon Q. Event Experience The workshop provided a clear view of the shift from traditional automation to Agentic AI. Speakers offered practical insights and directional guidance for Vietnam’s GenAI journey. The combination of AgentCore – Bedrock – AI-DLC, along with Amazon Q, forms a comprehensive enterprise AI foundation. Event Photos "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "AI/ML/GenAI on AWS I. General Event Information Event Name: AI/ML/GenAI on AWS Time: 8:30 AM – 12:00 PM, November 15, 2025 Location: AWS Vietnam Office Objectives: Provide foundational insights into AI/ML and Generative AI on AWS. Clarify the characteristics of foundation models and typical use cases. Introduce the AWS AI/ML service ecosystem—from ready-made services to customizable model-building platforms. Demonstrate how to build GenAI applications using Amazon Bedrock, RAG, and AgentCore. II. Agenda and Session Details 1. Opening Session – Introduction to GenAI and Foundation Models (8:30 – 9:00) Speaker Lam Tuan Kiet presented an overview of Generative AI and the role of foundation models:\nWhat is GenAI?\nUnlike traditional ML models centered on classification or prediction, GenAI can generate new content: text, images, audio, code, etc. Foundation models are trained on massive, multi-domain datasets and can later be fine-tuned or steered for specific applications. Foundation Models \u0026amp; Amazon Bedrock:\nBedrock is introduced as a centralized access platform for multiple foundation models (Claude, Llama, Titan…). AWS provides a unified API that enables experimentation and evaluation across models—allowing teams to choose the right one for tasks like chatbots, summarization, classification, or RAG. Prompt Engineering:\nDescribed as the most critical skill when working with GenAI—designing, testing, and refining prompts to guide model behavior. Three primary techniques: Zero-shot: instruct the model without examples. Few-shot: provide sample inputs/outputs so the model imitates the pattern. Chain-of-Thought: guide the model to express step-by-step reasoning for more logical outcomes. RAG (Retrieval-Augmented Generation):\nAn architecture that combines generative abilities with retrieval from enterprise knowledge bases. Key advantages: reduced hallucination, simple knowledge updates without retraining the model. 2. “AWS AI/ML Services Overview” \u0026amp; Embeddings/RAG in Action (9:00 – 10:30) This section highlights how AWS structures its AI/ML services and how they are applied to GenAI/RAG workloads.\n2.1. Embeddings and Titan Embeddings Embeddings are described as vector representations of text or images, enabling similarity search, clustering, and semantic retrieval.\nThey form the backbone of RAG, semantic search, and recommendation systems.\nTitan Embeddings were introduced as AWS’s optimized model for:\nSemantic search RAG Deep integration with Amazon Bedrock The demo illustrated a full RAG pipeline:\nData preprocessing → chunking → embedding generation → vector storage → querying → LLM response generation with contextual grounding. 2.2. Managed AI Services (Ready-made AI APIs) Speaker Hoang Anh introduced AWS’s fully managed AI services:\nAmazon Rekognition – Image/Video Recognition\nUse cases: object detection, facial recognition, text extraction Pricing: 0.0013 USD/image Amazon Translate – Neural Machine Translation\nPricing: 15 USD per 1M characters Amazon Textract – OCR and Structured Document Extraction\nPreserves tables, forms, key fields Pricing: 0.05 USD/page Amazon Transcribe – Speech-to-Text\nPricing: 0.024 USD/minute Amazon Polly – Text-to-Speech\nPricing: 4 USD per 1M characters Amazon Comprehend – NLP (sentiment, key phrase, entity recognition…)\nPricing: 0.0001 USD per 100 characters or 3 USD/hour Amazon Kendra – Semantic Search / RAG Support\nPricing: 30 USD/index/month + 0.35 USD/hour Amazon Personalize – Recommendation Service\nPricing includes training, data processing, and recommendation volume 3. “Generative AI with Amazon Bedrock” – AgentCore \u0026amp; Pipecat (10:45 – 12:00) This session dives into real-world GenAI architecture including voice agents and agentic systems.\n3.1. Pipecat – Framework for Voice/Multimodal Agents Pipecat is presented as a framework optimized for low-latency voice and multimodal assistants: It unifies STT, LLM reasoning, TTS, and third-party tools into a single coordinated pipeline. Designed for real-time interaction—ideal for callbots and voice agents. 3.2. Amazon Bedrock AgentCore and the Agentic AI Ecosystem Speaker Hieu Nghi explained AWS’s approach to multi-step AI agents:\nAgentic Systems Overview:\nLLMs respond; agents plan, call tools, retrieve data, and execute multi-step workflows. AgentCore capabilities:\nWorkflow orchestration Tool invocation (Lambda, internal APIs…) RAG integration with Bedrock Knowledge Bases Guardrails for content and access control Observability and monitoring for production workloads Supported agent frameworks include:\ncrew.ai, ADK, LlamaIndex, LangChain, LangGraph, Strands Agents SDK, etc. III. Key Insights and Learnings GenAI is only one component of a larger system\nYou still need RAG, data pipelines, search engines, and monitoring. Choosing between AWS managed AI and custom GenAI with Bedrock\nServices like Rekognition/Textract/Comprehend suit standardized use cases. For domain-specific data, Bedrock + Embeddings + RAG provides flexibility. Cost considerations must begin at the design phase\nPricing examples highlight the need to estimate usage volume early. Agentic thinking is the next evolution of GenAI\nBeyond Q\u0026amp;A—agents automate workflows, execute tasks, analyze data, and make operational decisions. IV. Application Plans Build a RAG pipeline on AWS\nTextract → Titan Embeddings → Vector Store → Kendra/Custom Search → Bedrock (Claude/Llama). Experiment with a small Bedrock AgentCore agent\nPhase 1: Retrieve + summarize documents. Phase 2: Call Lambda to automate actions. Improve prompt engineering skills\nPractice zero-shot, few-shot, CoT prompts for AI/ML, DevOps, and Security use cases. Build a reusable personal prompt library. V. Event Photos "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "DevOps on AWS I. General Information About the Event Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS Time: Monday, November 17, 2025, from 8:30 AM to 5:00 PM Location: AWS Vietnam Office Audience: Students, interns, and junior engineers interested in DevOps, system operations, and infrastructure automation on AWS. Main Objectives: Gain a clear understanding of the DevOps mindset and why DevOps/Platform Engineering has become a critical role in modern organizations. Learn the ecosystem of AWS DevOps Services (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and how to construct a full CI/CD pipeline. Become familiar with Infrastructure as Code (IaC) through CloudFormation and AWS CDK; understand why console-based operations become inefficient as systems scale. Build a structured understanding of container services on AWS, including ECR, ECS, EKS, and App Runner, along with appropriate deployment strategies. Understand concepts and tooling in Monitoring \u0026amp; Observability (CloudWatch, X-Ray, Grafana, Prometheus) to operate systems reliably and proactively. II. Detailed Agenda 2.1. Morning Session – DevOps Mindset \u0026amp; CI/CD \u0026amp; IaC 2.1.1. 8:30 – 9:00 | Welcome \u0026amp; DevOps Mindset (Quang Tinh – Platform Engineer)\nSpeaker Quang Tinh opened the session by connecting the content to the previous AI/ML workshop and setting the foundation for the day: DevOps exists to bridge the gap between Development and Operations.\nDevOps is not just a job title—it is a collaborative, automation-centric way of working.\nCore elements of DevOps culture emphasized:\nCollaboration: Dev and Ops work as one unified team with shared ownership of the product. Automation-first: prioritize automating build, test, and deployment processes. Measurement \u0026amp; Feedback: decisions must be based on observable metrics. Standard DORA metrics used to evaluate DevOps maturity:\nDeployment Frequency Lead Time for Changes MTTR (Mean Time to Recovery) Change Failure Rate The role of a Platform Engineer is described as building the shared deployment/infrastructure platform enabling teams to develop, test, and operate consistently and effectively.\n2.1.2. 9:00 – 10:30 | AWS DevOps Services – CI/CD Pipeline (Kha – CI/CD Workflow)\nSpeaker Kha provided a detailed walkthrough of how to build CI/CD pipelines using AWS native tools. The pipeline is broken down into four primary components: Source Control – AWS CodeCommit \u0026amp; Git strategies\nCodeCommit is AWS’s managed Git service with deep IAM integration. Two Git strategies discussed: GitFlow – suitable for structured release cycles with many branches. Trunk-based Development – supports continuous CI/CD with frequent small commits. Build \u0026amp; Test – AWS CodeBuild\nCodeBuild handles compilation and automated tests; workflows are defined in buildspec.yml. Produces artifacts used in deployment stages. Deployment – AWS CodeDeploy\nThree major deployment strategies: Blue/Green – highest safety margin. Canary – gradually shift traffic. Rolling update – update instances progressively. Orchestration – AWS CodePipeline\nConnects source → build → deploy into one automated sequence. Supports manual approvals, automated tests, and additional custom stages. The core theme: from commit to production, every step must be automated, observable, and verifiable. 2.1.3. 10:45 – 12:00 | Infrastructure as Code (IaC) – Thinh Nguyen \u0026amp; Hoang Anh\nSession opened with the question:\n“Why is ClickOps no longer suitable as systems scale?”\nMain reasons:\nAutomation: manual processes cannot be automated. Scalability: difficult to manage large numbers of resources. Reproducibility: nearly impossible to maintain identical environments across dev/staging/prod. Collaboration: no single source of truth. AWS CloudFormation – AWS’s native IaC service:\nConcepts: template, stack, stack update, drift detection. Advantages: version control, consistent multi-environment deployment, rollback on errors. AWS CDK (Cloud Development Kit):\nDefine infrastructure using programming languages (TypeScript, Python, Java…). Abstractions via constructs enable strong reuse. CDK synthesizes CloudFormation, combining flexibility of coding with IaC robustness. Conclusion:\nCloudFormation → best for low-level, AWS-native configurations. CDK → best for large projects requiring abstraction and reusability. 2.2. Afternoon Session – Container Services \u0026amp; Monitoring/Observability 2.2.1. 13:00 – 14:30 | Container Services on AWS (Tran Vi)\nBegan with the fundamentals of containers and the differences between containers vs virtual machines.\nIntroduction to the Docker workflow: Dockerfile → build → push → run.\nAmazon ECR:\nAWS-managed container registry with scanning, immutable tags, lifecycle policies. Amazon ECS:\nAWS’s native container orchestrator—simple and deeply integrated. Two execution modes: EC2 mode and Fargate mode (serverless). Key components: cluster, task, task definition, service. Amazon EKS:\nManaged Kubernetes—ideal for high flexibility and portability requirements. AWS App Runner:\nExtremely fast service deployment from source or container image without managing infrastructure. 2.2.2. 14:45 – 16:00 | Monitoring \u0026amp; Observability (Anh Nghiem, Anh Long, Anh Quy)\nExplanation of the difference between Monitoring and Observability.\nMain AWS tools: CloudWatch, X-Ray, Managed Grafana, Prometheus.\nCloudWatch:\nCollects metrics and logs, generates alarms, and builds operational dashboards. AWS X-Ray:\nEnables distributed tracing across microservices, identifies bottlenecks, supports debugging. Observability best practices:\nStandardize logs–metrics–traces, configure appropriate alerts, avoid alert fatigue. 2.2.3. 16:00 – 17:00 | DevOps Best Practices, Career \u0026amp; Q\u0026amp;A\nSummary of the day: mindset → IaC → CI/CD → containers → observability. Reinforced principles: automated testing, safe deployment patterns, fast rollback, blameless postmortems. Career guidance on AWS certifications for DevOps/Platform Engineering. III. Key Takeaways DevOps is a unified mindset, not just a toolset. A strong CI/CD pipeline ensures reliability and consistency from code to production. IaC is mandatory for scalable, maintainable infrastructure—ClickOps does not scale. AWS containers offer multiple layers of abstraction for varied workload complexity. Observability is essential for long-term operational stability. IV. Application Plan for Personal Projects \u0026amp; Learning Standardize DevOps workflow and create a template CI/CD pipeline. Move manual operations to IaC using CloudFormation/CDK. Containerize applications and experiment with ECS/App Runner deployment. Set up baseline monitoring with CloudWatch + X-Ray. Plan the certification path for DevOps/SRE on AWS. V. Event Photos "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "AWS Well-Architected Security Pillar I. General Information About the Event Event Name: AWS Well-Architected Security Pillar Time: 08:30 – 12:00, November 29, 2025 (morning only) Location: AWS Vietnam Office Main Objectives: Clarify the role of the Security Pillar within the AWS Well-Architected Framework and the Shared Responsibility Model. Summarize the five core security pillars: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, Incident Response. Analyze best practices, common mistakes, and real-world risks frequently encountered by organizations in Vietnam when using cloud services. Demonstrate hands-on examples including IAM policy simulation, detection-as-code, firewall configuration, and IR playbooks. II. Detailed Timeline 2.1. Opening \u0026amp; Security Foundation (08:30 – 08:50) The opening session established the foundation for security thinking on AWS:\nImportance of the Security Pillar within the Well-Architected Framework\nSecurity is not an “afterthought”; it must be built into the architecture from the design phase. Systems lacking proper security often face issues such as: Data leakage or unauthorized access. Service outages caused by misconfigurations or cyberattacks. Data loss or corruption due to missing backup or encryption layers. Traffic overload without adequate protection mechanisms. Core Security Principles:\nLeast Privilege: only grant the minimal required permissions. Zero Trust: assume no component is trustworthy by default. Defense in Depth: protect across multiple layers from identity to network to data. Shared Responsibility Model\nAWS is responsible for security OF the cloud. Customers are responsible for security IN the cloud, including IAM, resource configuration, data protection, logging, and incident response. Common Risks in the Vietnamese Market\nLong-lived access keys that become leaked or misused. Public S3 buckets without operator awareness. Servers or endpoints exposed to the public Internet unnecessarily. Insufficient or incomplete logging, making incident investigation difficult. 2.2. Pillar 1 – Identity \u0026amp; Access Management (08:50 – 09:30) This section focused on designing modern IAM systems suitable for multi-account architectures:\nIAM Users, Roles, Policies – eliminating long-lived credentials\nAvoid using IAM users with long-term access keys for workloads. Prefer IAM Roles with temporary tokens for improved security. Policies must follow least privilege, avoiding Action:* or Resource:*. IAM Identity Center + AWS Organizations\nIAM Identity Center (SSO) is the core of modern IAM architectures. Combined with AWS Organizations, it enables centralized governance with clearly separated accounts (prod, dev, security, logging…). SCP – Service Control Policies\nSCPs act as the “upper boundary” for permissions within an OU/account. IAM policies grant permissions; SCPs restrict the maximum possible scope. Permission Boundaries\nAllow teams to create/manage their own IAM users/roles within defined organizational limits. MFA, Credential Rotation, Access Analyzer\nEnforce MFA for the root user and privileged identities. Secrets Manager supports automatic credential rotation. IAM Access Analyzer helps detect unintended public or cross-account access. Mini Demo: IAM policy validation \u0026amp; access simulation\nDemonstrated using validation tools and simulators to verify behavior before deploying policies in production. 2.3. Pillar 2 – Detection \u0026amp; Continuous Monitoring (09:30 – 09:55) This segment emphasized observation – detection – alerting:\nAWS CloudTrail (Duc Anh)\nLogs every API call in the system: Management events Data events (S3 object access, Lambda invoke) Network activity Supports multi-account logging when enabled at the Organization level. Integrated with EventBridge for detection-as-code automation workflows. Amazon GuardDuty (Tuan Thinh)\nDetects anomalies using ML and baselines using: VPC Flow Logs CloudTrail DNS logs Advanced protection packages include EKS, RDS, and Lambda runtime monitoring. Can isolate compromised resources automatically via EventBridge. AWS Security Hub (Thanh Dat)\nAggregates security findings across services. Normalizes results into ASFF and evaluates against CIS \u0026amp; AWS best practices. Supports detection-as-code across multi-account environments. 2.4. Pillar 3 – Infrastructure Protection (10:10 – 10:40) Focused on network security and traffic control:\nCommon network attack vectors\nInbound, outbound, and east–west traffic between workloads. Security Groups\nStateful; return traffic is automatically allowed. Only support allow rules (no deny). Allow SG sharing and referencing across VPCs. Network ACLs\nStateless, applied at subnet level. Support allow + deny, suitable for coarse-grained rules. DNS \u0026amp; Perimeter Protection – Route 53\nPrivate DNS, DNS filtering, rule management. Applicable to cloud-only and hybrid deployments. AWS Network Firewall\nControls traffic across multi-VPC environments. Supports stateless + stateful rules. Works well with Transit Gateway. Edge Protection: AWS WAF + AWS Shield\nProvides DDoS protection and malicious request filtering. 2.5. Pillar 4 – Data Protection (10:40 – 11:10) Covered best practices for protecting data at rest and in transit:\nAWS KMS\nExplained master key vs data key encryption workflow. Key policies strictly control access—even admins cannot use a key unless granted. Difference between AWS-managed keys and CMKs with custom policies and rotation. Data Classification \u0026amp; Guardrails\nAmazon Macie automatically identifies sensitive data (PII) in S3. Guardrails can deny operations when objects are not encrypted. Encryption in Transit\nTLS/SSL for S3, DynamoDB, RDS. EBS/Nitro secure transport encryption. ACM for certificate management. Secrets Manager\nSecure storage for credentials, tokens, connection strings. Automated rotation via Lambda integration. 2.6. Pillar 5 – Incident Response (11:10 – 11:40) Focused on structured and automated IR workflows:\nModern context\nMulti-account + hybrid architectures → manual operations are insufficient. Common incidents: credential leaks, S3 public exposure, malware, service outages… Security Responsibilities are Shared\nBaseline services recommended: AWS Organizations + SCP CloudTrail AWS Config GuardDuty Security Hub Prevention Guidelines\nRemove long-lived access keys. Avoid exposing S3 unless strictly required. Minimize public Internet exposure for workloads. Apply all infrastructure changes via IaC. Add approval layers for high-risk changes. AWS Incident Response Framework\nPrepare Detect \u0026amp; Analyze Contain Eradicate \u0026amp; Recover Post-Incident Review Playbook examples\nCompromised IAM key Public S3 bucket Malware-infected EC2 instance Workflows include: isolation → snapshot → revoke → automation with Lambda/Step Functions. 2.7. Wrap-Up \u0026amp; Q\u0026amp;A (11:40 – 12:00) Recap of the five security pillars. Highlighted common mistakes by Vietnamese organizations: Poorly designed IAM. Missing or incomplete CloudTrail/GuardDuty/Security Hub setup. Accidental S3 public exposure. Recommended learning path: aim for AWS Security Specialty. III. Key Takeaways Security must be a design principle, not an afterthought. Modern IAM requires multi-account structure + SSO + SCP + rotation + MFA. Detection-as-code is essential for complex cloud environments. Infrastructure Protection requires layered network design—not just firewalls. Data Protection and IR complete the full security posture. IV. Action Plan for Learning \u0026amp; Personal Projects Standardize IAM with multi-account design following AWS best practices. Enable CloudTrail, GuardDuty, and Security Hub as baseline detection layers. Redesign network architectures using segmentation to minimize public exposure. Apply KMS + Secrets Manager + Macie for sensitive data workflows. Strengthen Security Pillar knowledge to pursue AWS Security Specialty. V. Event Photos "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Minh Nhat\nPhone Number: 078 555 1708\nEmail: nhatm2400@gmail.com\nUniversity: FPT University HCMC\nMajor: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 00/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Goals: Connect and get familiar with members in the First Cloud Journey. Understand basic AWS services and how to use the console. Learn how to draw AWS architecture diagrams. Grasp fundamental AWS concepts. Tasks to be completed this week: Day Task Start Date Completion Date Reference Source 2 - Connect with FCJ members - Read and take note of internship regulations and rules - Learn about how Workshop tasks are executed 08/09/2025 08/09/2025 https://policies.fcjuni.com/ 3 - Explore AWS and its service categories + Compute + Storage + Networking + Database 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Management Console - Manage costs using AWS Budget Practice: + Create AWS account + Enable MFA for AWS Account + Verify account + Explore and configure AWS Management Console 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/vi/5-explore-and-configure-the-aws-management-console/ https://000007.awsstudygroup.com/vi/ 5 - Learn how to draw AWS architecture diagrams using draw.io - Practice: Draw the architecture diagrams assigned in the lab 11/09/2025 11/09/2025 https://www.youtube.com/watch?v=l8isyDe-GwY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=3/ 6 - Study AWS Virtual Private Cloud concepts, Subnets; Elastic Network Interface; Internet Gateway and NAT Gateway - Learn concepts such as Security Group; Transit Gateway; VPN 12/08/2025 12/08/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=25 https://www.youtube.com/watch?v=BPuD1l2hEQ4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=26 https://www.youtube.com/watch?v=CXU8D3kyxIc\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=27 Week 1 Achievements: Understood what AWS is and learned the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nGot familiar with the AWS Management Console and learned how to locate, access, and use services from the web interface.\nInstalled and configured AWS on the computer, including:\nMFA Budget Region Used AWS CLI to perform basic operations such as:\nChecking account information \u0026amp; configuration Listing regions Viewing EC2 services Checking running services Learned how to draw AWS architectures using draw.io.\nGained fundamental knowledge of AWS Private Cloud and concepts such as Subnet; Elastic Network Interface; Internet Gateway; NAT Gateway; Security Group; Transit Gateway; and VPN.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Goals: Dive deeper into AWS services. Practice deploying basic AWS features. Tasks to be carried out this week: Day Task Start Date Completion Date References 2 - Learn basic networking with Amazon Virtual Private Cloud - Hands-on: + Deploy Amazon EC2 Instances + Learn how to configure Site-to-Site VPN 15/09/2025 15/09/2025 https://000003.awsstudygroup.com/vi/5-vpnsitetosite/ 3 - Learn basic knowledge about virtual servers with Amazon Elastic Compute Cloud (EC2) - Hands-on: + Launch Windows/Linux Instances + Deploy a Node.js application on Amazon EC2 Windows/Amazon Linux + Explore deeper into Amazon EC2 + Deploy the AWS User Management application on Amazon EC2 running Windows Server 2022 16/09/2025 16/09/2025 https://000004.awsstudygroup.com/vi/ 4 - Learn how to develop applications on the cloud with AWS Cloud9 - Learn how to grant application permissions with IAM Roles for EC2 - Hands-on: + Use basic AWS Cloud9 operations such as command line, text files, etc. + Create environment for the workshop 17/09/2025 17/09/2025 https://000049.awsstudygroup.com/vi/ https://000048.awsstudygroup.com/vi/ 5 - Deploy a Website using Amazon S3 - Hands-on: + Perform basic operations with Amazon S3 such as creating a bucket, configuring attributes like Block Public Access, enabling Static Website Hosting, moving and copying objects 18/09/2025 18/09/2025 https://000057.awsstudygroup.com/vi/ 6 - Team meeting to brainstorm strategies, project ideas, and divide groups for implementation 19/09/2025 19/09/2025 Week 2 Achievements: Gained a deeper understanding of AWS core services, especially:\nAmazon VPC (Virtual Private Cloud) Amazon EC2 (Elastic Compute Cloud) AWS Cloud9 Amazon S3 IAM Roles and access management Successfully practiced:\nCreating and configuring Amazon VPC and Subnets Launching EC2 Instances running Windows and Linux Deploying a Node.js application on EC2 (Windows \u0026amp; Linux) Deploying the AWS User Management application on EC2 running Windows Server 2022 Creating and configuring IAM Roles for EC2 Using AWS Cloud9 for development, running CLI commands, and editing files directly in the cloud environment Hosting a static Website with Amazon S3, including bucket configuration, enabling Static Website Hosting, and managing object permissions Learned how to combine the web console and AWS CLI to manage resources effectively.\nParticipated in team meetings to discuss strategies, project ideas, and divide groups for task implementation.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Get familiar with deploying and managing relational databases on Amazon RDS. Practice deploying applications with EC2 Auto Scaling Group and Elastic Load Balancing. Use Amazon CloudWatch to monitor the system (Metrics, Logs, Alarms) and practice connecting DNS Hybrid. Master operations with AWS CLI, from installation to initializing basic services (EC2, S3, IAM, VPC, …). Learn and practice the main features of Amazon DynamoDB. Tasks to be implemented this week: Day Task Start Date End Date Reference Materials 2 - Get familiar with deploying and managing relational databases on RDS. - Practice: + Create an RDS database instance. + Deploy the application and fix errors. 22/09/2025 22/09/2025 https://000005.awsstudygroup.com/vi/ 3 - Practice deploying applications using Amazon EC2 Auto Scaling Group. - Team meeting to report progress, project. - Practice: + Create EC2 Auto Scaling Group. + Deploy Elastic Load Balancing. 23/09/2025 23/09/2025 https://000006.awsstudygroup.com/vi/ 4 - Practice with Amazon CloudWatch including features such as CloudWatch Metrics, CloudWatch Logs, CloudWatch Alarms. - Build a DNS Hybrid system. - Practice: + Test CloudWatch features. + Run and connect to RDGW and set up DNS. 24/09/2025 24/09/2025 https://000008.awsstudygroup.com/vi/ https://000010.awsstudygroup.com/vi/ 5 - Learn about AWS Command Line Interface (AWS CLI) and use it to interact with AWS services. - Practice: + Install AWS CLI. + Initialize basic services such as EC2, S3, IAM, VPC,… via AWS CLI. 25/09/2025 25/09/2025 https://000011.awsstudygroup.com/vi/ 6 - Team meeting to report progress. - Learn about Amazon DynamoDB. - Practice: + Practice DynamoDB features. 26/09/2025 26/09/2025 https://000060.awsstudygroup.com/vi/ Week 3 Achievements: Successfully created and managed RDS database instance, deployed the application, and fixed errors during installation. Practiced deploying EC2 Auto Scaling Group with Elastic Load Balancer to optimize resources. Used CloudWatch to monitor Metrics, Logs, create basic Alarms, and tested DNS Hybrid setup. Installed and configured AWS CLI (Access Key, Secret Key, default Region, …). Worked with AWS CLI to: Check account information \u0026amp; configuration. Retrieve the list of regions. Manage EC2, S3, IAM, VPC, Key Pair,… Practiced basic operations with Amazon DynamoDB and understood how to manage NoSQL data. Attended team meetings to report progress and connect with team members. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Goals: Gain knowledge and hands-on experience with Amazon ElastiCache (Redis) Learn the fundamentals and practice AWS Networking services (VPC, VPN Site-to-Site, Transit Gateway, Route 53, VPC Peering, VPC Endpoints) Learn how to host static web content using Amazon S3 and CloudFront Practice deploying WordPress on AWS using an EC2 architecture (public subnet) connected to a Database (private subnet) Participate in team meetings to brainstorm project ideas and assign tasks Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Learn about Amazon ElastiCache - REDIS. - Practice: + Create an ElastiCache cluster. + Use AWS SDK to write and read data from ElastiCache 29/09/2025 29/09/2025 https://000061.awsstudygroup.com/vi/ 3 - Learn the fundamentals of AWS Networking services - Study Networking theories - Practice: + Install and deploy services such as VPN Site-to-Site, Transit Gateway, Route53 DNS, VPC Endpoints, VPC Peering 30/09/2025 30/09/2025 https://000092.awsstudygroup.com/vi/ 4 - Learn how to store static web content in Amazon S3 Bucket and Amazon CloudFront - Practice: + Perform the steps to host static web including creating an S3 bucket and configuring Amazon CloudFront 01/01/2025 01/10/2025 https://000094.awsstudygroup.com/vi/s 5 - Learn and practice deploying WordPress on AWS Cloud - Practice: + Create and install WordPress on an EC2 Instance in a public subnet connected to a Database in a private subnet within the VPC 02/10/2025 02/10/2025 https://000101.awsstudygroup.com/vi/ 6 - Team meeting to report work progress. - Review lessons learned over the past 4 weeks. 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Learned and practiced deploying Amazon ElastiCache (Redis), using AWS SDK to read/write data. Understood and practiced AWS Networking services including VPN Site-to-Site, Transit Gateway, Route 53, VPC Peering, and VPC Endpoints. Successfully deployed static website hosting using Amazon S3 Bucket and CloudFront. Installed and configured WordPress on AWS Cloud, connecting EC2 (public subnet) with a Database in a private subnet. Participated in team meetings to report project and work progress. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Goals: Master the Migration process: Understand and practice transferring virtual machines between On-premise environments and AWS Cloud. Optimize operations: Implement automation to reduce infrastructure costs and manage system resources efficiently. System monitoring: Gain proficiency with Grafana for data visualization combined with AWS CloudWatch. Project development: Finalize the proposal and initial architecture for the team project. Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Translate 3 blog posts - Review the Proposal and draft the architectural outline 06/10/2025 06/10/2025 3 - Weekly team meeting: assign features for implementation. - Learn the overview of AWS VM Import/Export. - Study theoretical concepts: supported formats, IAM requirements, S3, ACL. Practice: + Import a virtual machine from on-premise to AWS + Create AMI and launch EC2 from the imported VM + Export AMI/EC2 instance from AWS back to VM format 07/10/2025 07/10/2025 https://000014.awsstudygroup.com/vi/ 4 - Learn cost-optimization solutions for EC2 using AWS Lambda. - Study architecture: Lambda, EventBridge, IAM Role, EC2 Tag. Practice: + Configure Lambda to automatically Start/Stop EC2. + Create IAM Role for Lambda. + Configure EventBridge Rule. + Integrate Slack Webhook for notifications. 08/10/2025 08/10/2025 https://000022.awsstudygroup.com/vi/ 5 - Learn the overview of Grafana on AWS and system monitoring. - Study theory: Grafana architecture, CloudWatch, IAM Role/User, Security Group. Practice: + Deploy VPC, Subnet, and Security Group for Grafana. + Create EC2 instance and attach IAM Role for CloudWatch access. + Install Grafana on EC2 and configure the service. + Add CloudWatch Data Source and create a CPU monitoring Dashboard. 09/10/2025 09/10/2025 https://000029.awsstudygroup.com/vi/ 6 - Weekly team meeting. - Learn the overview of Tag and Resource Groups in AWS. - Study theory: Tag definitions, metadata usage, Resource Groups mechanism. Practice: + Create EC2 instance with Tag and manage Tags on Console. + Filter and search resources via Tag. + Add/Remove Tags using AWS CLI. + Create a Tag-based Resource Group and manage grouped resources. 10/10/2025 10/10/2025 https://000027.awsstudygroup.com/vi/ Week 5 Achievements: Project: Reviewed and built the Proposal, collaborated with the team to design the initial architecture, and completed translation of three technical blog posts as planned. Migration: Mastered and successfully practiced Import/Export of virtual machines (VMDK/OVA formats) between On-premise and AWS environments using S3 and AMI. Automation: Implemented a cost-optimization solution using AWS Lambda and EventBridge to automatically Start/Stop EC2 on schedule, with Slack Webhook notifications. Monitoring: Successfully built a monitoring system with Grafana on EC2, configured CloudWatch Data Source, and visualized performance metrics (CPU/Network) on Dashboards. Management: Gained proficiency in large-scale resource management through Tagging and Resource Groups, including tagging, searching, and managing metadata via both Console and AWS CLI. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Goals: Resource management and classification: Master tagging strategies (Tagging Strategy) to manage resources efficiently and organize Resource Groups. Advanced security: Understand and practice attribute-based access control (ABAC) using IAM Policies combined with Tags and Region restrictions. Infrastructure automation (IaC): Gain proficiency with AWS CloudFormation to deploy, manage, and replicate infrastructure through code (Templates, Stacks, StackSets). Project progress: Finalize the Proposal and begin designing the Backend architecture. Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Continue refining the proposal 13/10/2025 13/10/2025 3 - Learn the mechanism of Tag and Resource Groups in AWS. - Study theory: Tag concepts, purpose of resource classification, how Resource Groups work. Practice: + Create EC2 instance with Tag and manage Tags on Console. + Add/Remove Tags and filter resources by Tag. + Assign Tags using AWS CLI and search resources via filters. + Create Tag-based Resource Group and view grouped resources. 14/10/2025 14/10/2025 https://000027.awsstudygroup.com/vi/ 4 - Learn access control for EC2 using IAM and Resource Tags. - Study theory: IAM Least Privilege, IAM Conditions, Region and Tag restrictions. Practice: + Create 5 IAM Policies to control EC2 permissions by Tag and Region. + Create IAM Role ec2-admin-team-alpha and attach policies. + Switch Role and test: create EC2 with correct/incorrect Tag, access allowed/denied Regions, modify Tags, and manage Instances. + Delete Role and Policies after completion. 15/10/2025 15/10/2025 https://000028.awsstudygroup.com/vi/ 5 - Weekly team meeting: assign tasks. - Learn the overview of AWS CloudFormation and Infrastructure as Code. - Study theory: template structure, intrinsic functions, pseudo parameters, Cloud9, Custom Resources, Mappings, StackSets, and Drift Detection. Practice: + Create IAM User/Role and set up the Cloud9 environment. + Write and lint a basic EC2 template, deploy the stack and check Outputs. + Create a Custom Resource using Lambda to generate SSH Key. + Create StackSets to deploy EC2 across multiple regions and perform Drift Detection. 16/10/2025 16/10/2025 https://000037.awsstudygroup.com/vi/ 6 - Weekly team meeting. - Begin researching Backend architecture for the Project 17/10/2025 17/10/2025 Week 6 Achievements: Project: Developed the Proposal and researched foundational Backend architecture. Management: Gained solid understanding of large-scale resource classification through Tagging Strategy and Resource Groups. Security: Successfully implemented advanced access control using IAM Policies with conditions based on Tags and Region. CloudFormation: Achieved proficiency in the IaC workflow—writing Templates, creating Custom Resources with Lambda, multi-region deployment (StackSets), and performing Drift Detection. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Goals: Continue refining the architecture Gain a basic understanding of the Project Backend Participate in team meetings to report progress Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Refine architectural details. 11/08/2025 11/08/2025 3 - Learn how to develop a Backend Project. 12/08/2025 12/08/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 4 - Team meeting: Report findings about Backend architecture. 13/08/2025 13/08/2025 5 - Prepare midterm review materials. 14/08/2025 15/08/2025 6 - Weekly team progress meeting. 15/08/2025 15/08/2025 Week 7 Achievements: Gained a basic understanding of the project\u0026rsquo;s Backend architecture Reported progress to the Team "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Goals: Review and prepare for the midterm exam based on previously learned and practiced knowledge. Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Review Secure Architectures: IAM, MFA, SCP, Encryption (KMS, TLS/ACM), Security Groups, NACLs, GuardDuty, Shield, WAF, Secrets Manager. 20/10/2025 20/10/2025 3 - Study Resilient Architectures: Multi-AZ, Multi-Region, DR Strategies, Auto Scaling, Route 53, Load Balancing, Backup \u0026amp; Restore. 21/10/2025 21/10/2025 4 - Study High-Performing Architectures: Compute scaling (EC2 Auto Scaling, Lambda, Fargate), Storage (S3, EFS, EBS), Caching, Network Optimization (CloudFront, Global Accelerator). 22/10/2025 22/10/2025 5 - Study Cost-Optimized Architectures: Cost Explorer, Budgets, Savings Plans, Lifecycle Policies, NAT Gateway Optimization, Storage Tiering. 23/10/2025 23/10/2025 6 - Weekly team meeting. - General review of all study topics. 24/10/2025 24/10/2025 Week 8 Achievements: Secure Architectures: Mastered AWS multi-layer security mechanisms:\nIdentity and access management (IAM, MFA, SCP). Network protection (Security Groups, NACLs, WAF, Shield). Data encryption and protection (KMS, Secrets Manager). Resilient Architectures: Understood key strategies for building highly available and durable systems:\nHigh Availability (Multi-AZ) and Global Scale (Multi-Region). Disaster Recovery strategies (DR) and load balancing (ELB, Route 53). High-Performing Architectures: Learned how to choose optimized resources for performance:\nScaling mechanisms (Auto Scaling, Serverless/Lambda). Storage solutions (EBS, S3, EFS) and caching/CDN (CloudFront). Cost-Optimized Architectures: Gained knowledge of effective cost management strategies:\nUsing monitoring tools (Cost Explorer, Budgets). Data lifecycle optimization (S3 Lifecycle) and pricing models (Savings Plans). "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Goals: System Design: Build the Backend architecture and model the project’s Data Flow. Data Layer Deployment: Set up storage and database infrastructure on AWS (S3, DynamoDB). Security \u0026amp; Operations: Configure IAM and necessary security policies in preparation for Deployment. Project Development: Collaborate with the team to align technical solutions and track progress. Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Learn about the required infrastructure for the Backend Project. - Learn how to develop a Backend Project. 27/10/2025 27/10/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 3 - Design the Backend system. - Draw the Data Flow diagram. - Team meeting to report project progress. 28/10/2025 28/10/2025 4 - Learn about databases. - Set up basic Databases: S3 bucket and DynamoDB. - Create IAM User for Deployment. 29/10/2025 29/10/2025 5 - Learn how to set up IAM \u0026amp; Security. - Continue learning Backend development. 30/10/2025 30/10/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 6 - Team meeting to report Project progress. 31/10/2025 31/10/2025 Week 9 Achievements: Architecture: Completed the Backend system design and Data Flow Diagram, aligned the technical solution with the team. Database \u0026amp; Storage: Successfully created an Amazon S3 Bucket for storing static assets/files. Set up basic Amazon DynamoDB tables to support application functionality. Security (IAM): Created and configured a dedicated IAM User for Deployment. Applied fundamental security policies to control access to AWS resources. Backend Foundation: Gained solid understanding of required infrastructure and the Backend development workflow on AWS. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nOn this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Goals: Project Initialization: Set up the Backend development environment and define a standardized directory structure for the Node.js project. Core Development: Build and implement the Chatbot processing logic using AWS Lambda. Integration \u0026amp; Debugging: Resolve technical issues related to environment configuration and data connectivity with DynamoDB. Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Initialize Backend Project. - Set up Node.js project. - Design the Backend folder structure. - Learn how to develop a Backend Project. 03/11/2025 03/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 3 - Initialize a basic Chatbot Lambda Function. - Run and test the Lambda function. - Learn Backend development. - Team meeting to report progress. 04/11/2025 04/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 4 - Initialize a basic Chatbot Lambda Function. - Run and test the Lambda function. - Learn Backend development. 05/11/2025 05/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 5 - Research and fix issues such as Environment Variables and DynamoDB data mapping errors. - Learn Backend development. 06/11/2025 06/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 6 - Team meeting to report project progress. 07/11/2025 07/11/2025 Week 10 Achievements: Backend Setup: Successfully initialized the Node.js project and completed directory structuring for scalable development. Serverless Function: Built and deployed the AWS Lambda Function supporting Chatbot features. Tested and verified the Lambda Function’s behavior and execution. Troubleshooting: Successfully fixed Environment Variable configuration issues. Resolved data mapping inconsistencies between the application and DynamoDB. Project Progress: Maintained steady Backend development and provided regular updates during team meetings. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Goals: Core Feature Development: Focus on building and refining the logic for key Lambda Functions related to AI (RAG, Analyzer, Generator). Architecture Completion: Finalize the Backend source code structure to ensure organization and scalability. Optimization \u0026amp; Debugging: Address performance issues typical in Serverless environments (Timeout, Cold Start) when integrating with LLMs. Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Research and support adjustments for the RAG SEARCH Lambda Function. - Learn Backend development. 10/11/2025 10/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 3 - Research and support adjustments for the LLM ANALYZER Lambda Function. - Learn Backend development. - Team meeting to report progress. 11/11/2025 11/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 4 - Research and support adjustments for the GENERATE CONTRACT Lambda Function. - Learn Backend development. 12/11/2025 12/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 5 - Complete foundational structure and content for Backend folders. - Continue learning Backend development. 13/11/2025 13/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 6 - Team meeting to report ongoing progress. - Fix Lambda-related issues such as Timeout, Cold Start, LLM behavior, etc. 14/11/2025 14/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo Week 11 Achievements: Serverless AI Functions: Successfully developed and refined three core functions: RAG Search: Contextual information retrieval. LLM Analyzer: Analyze and evaluate contract content. Generate Contract: Automatically generate draft contract documents. Backend Maturity: Completed the Backend folder structure and foundational content, preparing for further integrations. Performance Tuning: Timeout issues for long-running LLM tasks remain unresolved. Mitigated the impact of Cold Start on user experience. Fixed various logic errors during LLM integration and testing. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Goals: System Integration: Fully connect the Backend with AWS Lambda Functions (AI services) and resolve compatibility issues. Infrastructure Setup: Configure essential security services (Cognito, Secrets Manager) and monitoring tools (CloudWatch). Full-stack Deployment: Integrate Frontend – Backend and deploy the complete application using AWS Amplify. QA \u0026amp; Bug Fixing: Ensure smooth end-to-end workflows and fix logic issues related to Authentication, Database, and AI Editor. Tasks to be completed this week: Day Task Start Date Completion Date References 2 - Integrate Lambda ARNs into Backend code. - Fix issues such as Backend–Lambda incompatibility. - Learn Backend development. 17/11/2025 17/11/2025 https://www.youtube.com/watch?v=KAV8vo7hGAo 3 - Add complete configurations. - Run and test Backend features using Postman (generate, upload contract, chatbot, rag, etc.). - Fix remaining logic issues. 18/11/2025 18/11/2025 4 - Set up Cognito, Secrets Manager, CloudWatch, etc. - Re-test Backend and validate all features. - Fix remaining logic issues. 19/11/2025 19/11/2025 5 - Connect Backend with Frontend. - Link workflows and deploy to Amplify. - Fix remaining logic issues. 20/11/2025 20/11/2025 6 - Team meeting for final result reporting. - Fix issues such as password change failures, contract template not saving, and AI not working in editor. - Fix logic errors related to database and Cognito. 21/11/2025 21/11/2025 Week 12 Achievements: Backend \u0026amp; Serverless Integration:\nSuccessfully integrated ARNs of Lambda Functions (RAG, Chatbot, Generator) into the Backend. Fully resolved environment compatibility issues between Node.js Backend and AWS Lambda. Fixed logic issues arising from integrating Large Language Models (LLM). Security \u0026amp; Monitoring:\nSuccessfully configured Amazon Cognito for User Pool/Identity Pool. Set up AWS Secrets Manager to securely manage environment variables and CloudWatch for system logging. Functional Testing:\nSuccessfully validated all core features via Postman: contract upload, RAG search, AI chatbot, and contract generation. Deployment \u0026amp; Bug Fixing:\nSuccessfully connected Frontend with Backend. Deployed the full application to AWS Amplify. Completed major business logic fixes: password reset (Cognito), contract template saving (Database), and AI execution in editor. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/3-blogstranslated/",
	"title": "Translated Blog Posts",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Accelerating the Quantum Toolkit for Python (QuTiP) with cuQuantum on AWS This blog describes the collaborative effort between Université de Sherbrooke, NVIDIA, and AWS to accelerate quantum simulations by integrating cuQuantum into the QuTiP framework. Using the new plugin on Amazon EC2 instances powered by NVIDIA H200 GPUs, the team achieved up to a 4,000× speedup for a 64-qubit system compared to CPU-based simulations. This improvement enables deeper research into complex multi-qubit interactions needed to optimize next-generation quantum hardware. The open-source solution is now available on PyPi for the broader research community.\nBlog 2 - New general-purpose Amazon EC2 M8i and M8i-flex instances are now available This blog introduces the EC2 M8i and M8i-flex instances powered by Intel Xeon 6 processors, offering higher performance and better price efficiency than M7i. They deliver up to 20% more performance, 2.5× higher memory bandwidth, and notable gains for NGINX, PostgreSQL, and AI recommendation models. The 6th-generation Nitro Cards provide double the network and EBS bandwidth, along with flexible bandwidth allocation. These new instances suit general-purpose workloads and are available across multiple AWS Regions with On-Demand, Savings Plans, and Spot purchase options.\nBlog 3 - New courses and certification updates from AWS Training and Certification in August 2025 This blog highlights new AWS training courses and certification updates for August 2025, including nine new digital courses on Skill Builder, localized exam prep content for AI Practitioner and Cloud Practitioner, and a major AWS Jam update allowing all Skill Builder Team members to create Jam events. AWS also expanded language support for foundational certification exams, introduced multilingual exam prep plans, and launched a 1-day Agentic AI Foundations course that teaches core principles for designing agentic AI systems on AWS.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/4-eventparticipated/",
	"title": "Events Attended",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in four events—each offering valuable knowledge, memorable experiences, and exciting gifts and moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nTime: Thursday, September 18, 2025, 9:00 – 17:00\nLocation: Ho Chi Minh City\nRole: Attendee\nEvent Summary:\nVietnam Cloud Day 2025 provided a comprehensive view of AWS’s GenAI strategy in Vietnam, opening with a keynote that emphasized Agentic AI and a unified data foundation as the backbone for large-scale AI applications. Insights from Techcombank, U2U Network, and the “GenAI Revolution” panel highlighted how enterprises are adopting GenAI for operations, system modernization, and risk governance aligned with Responsible AI principles. The technical track focused on Migration \u0026amp; Modernization, VMware-to-AWS transformation, and an Amazon Q Developer demo showcasing SDLC automation. Key topics such as Bedrock, Nova, AgentCore, AI-DLC, and the Lake House data ecosystem helped shape an end-to-end perspective for building and operating GenAI systems on AWS.\nEvent 2 Event Name: AI/ML/GenAI Workshop on AWS\nTime: Saturday, November 15, 2025, 8:30 – 12:00\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Summary:\nThe event focused on the overall landscape of AI/ML and Generative AI on AWS, introducing service layers ranging from AI Services to Amazon SageMaker and Amazon Bedrock. The session highlighted foundation models (Claude, Llama, Titan), prompt engineering techniques, along with RAG and Bedrock Agents for enterprise Q\u0026amp;A systems. The accompanying workshop guided participants through SageMaker for traditional ML and Bedrock for GenAI, illustrating the differences between the two approaches and how combining Bedrock + embeddings + RAG reduces hallucinations while maximizing value from enterprise data.\nEvent 3 Event Name: DevOps Workshop on AWS\nTime: Monday, November 17, 2025, 8:30 – 17:00\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Summary:\nThe DevOps on AWS workshop delivered a systematic overview of DevOps principles and the AWS operational toolchain, covering CI/CD with CodeCommit–CodeBuild–CodeDeploy–CodePipeline, IaC with CloudFormation and CDK, container services (ECR, ECS, EKS, App Runner), and monitoring/observability via CloudWatch and X-Ray. The workshop explored automated build–test–deploy workflows, infrastructure-as-code design, and real-world microservices deployment and monitoring. The content clarified the advantages of IaC over ClickOps, how to choose suitable container runtimes, and why observability becomes essential as systems scale—laying a strong foundation for designing CI/CD pipelines and deployment environments for AI/ML and web applications.\nEvent 4 Event Name: AWS Well-Architected Security Pillar Workshop\nTime: Saturday, November 29, 2025, 8:30 – 12:00\nLocation: AWS Vietnam Office\nRole: Attendee\nEvent Summary:\nThis workshop focused on the Security Pillar of the AWS Well-Architected Framework, covering all five domains: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. The content emphasized modern IAM architecture, logging and detection-as-code, multi-layer network protection, encryption and secrets management, and structured incident response processes. The workshop reinforced the concept of “security-by-design,” the need to establish baseline security controls, and the use of automated playbooks to ensure system safety and resilience.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/nhatnm.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]